{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# ============================================================\n","# 0. Configurações de Otimização do Fabric\n","# ============================================================\n","# Habilita o V-Order (padrão no Fabric, mas garante escrita otimizada)\n","spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\") # ~1GB por arquivo se possível\n","\n","# ============================================================\n","# 1. Caminho físico\n","# ============================================================\n","path_destino = (\n","    \"abfss://ws_departamento_pessoal@onelake.dfs.fabric.microsoft.com/\"\n","    \"lk_departamento_pessoal.Lakehouse/Tables/tab_gold_dim_funcionario_ultima_posicao_mes\"\n",")\n","\n","# ============================================================\n","# 2. Ler e Filtrar\n","# ============================================================\n","df_historico = spark.table(\"tab_gold_dim_funcionario_historico\")\n","\n","df_filtrado = df_historico.filter(col(\"RK_MES\") == 1)\n","\n","# ============================================================\n","# 3. CORREÇÃO DO SKEW\n","#\n","#    Usando apenas o número, o Spark faz um Round-Robin,\n","#    distribuindo os dados igualmente independente do conteúdo.\n","# ============================================================\n","NUM_PARTICOES = 200  # Se tiver menos de 10GB de dados, pode até baixar para 20-50\n","\n","df_reparticionado = df_filtrado.repartition(NUM_PARTICOES)\n","\n","# ============================================================\n","# 4. Escrever\n","# ============================================================\n","(\n","    df_reparticionado.write\n","        .format(\"delta\")\n","        .mode(\"overwrite\")\n","        .option(\"overwriteSchema\", \"true\") # Boa prática se a estrutura mudar\n","        .save(path_destino)\n",")\n","\n","print(f\"Gravação concluída. Partições utilizadas: {NUM_PARTICOES}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"54e73718-e4f6-4dab-b0f4-7279a9668f11","normalized_state":"finished","queued_time":"2025-12-02T19:34:01.456441Z","session_start_time":null,"execution_start_time":"2025-12-02T19:34:01.4576312Z","execution_finish_time":"2025-12-02T19:34:15.369772Z","parent_msg_id":"46aca669-90e1-4c2b-894b-bc8664d19bb9"},"text/plain":"StatementMeta(, 54e73718-e4f6-4dab-b0f4-7279a9668f11, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Gravação concluída. Partições utilizadas: 200\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"cellStatus":"{\"Suporte BI\":{\"session_start_time\":null,\"execution_start_time\":\"2025-12-02T19:34:01.4576312Z\",\"execution_finish_time\":\"2025-12-02T19:34:15.369772Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}"},"id":"f625b998-4365-4ea7-89b6-2927afc92ca4"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"2df79f82-2938-4b29-9d25-58c6ffa4aba7"}],"default_lakehouse":"2df79f82-2938-4b29-9d25-58c6ffa4aba7","default_lakehouse_name":"lk_departamento_pessoal","default_lakehouse_workspace_id":"4665fc94-ec39-4b0d-a3cf-570d42f500cc"}}},"nbformat":4,"nbformat_minor":5}