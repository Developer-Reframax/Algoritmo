{"cells":[{"cell_type":"code","source":["import time\n","import math\n","import builtins  # para garantir o uso do max() nativo do Python\n","from pyspark.sql.functions import col\n","from pyspark import StorageLevel\n","from delta.tables import DeltaTable\n","\n","# ============================================================\n","# 0. Configurações de Segurança e Performance\n","# ============================================================\n","\n","# Desliga a otimização automática de escrita do Fabric para respeitar\n","# nosso particionamento manual (Round Robin).\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"false\")\n","\n","# Habilita Adaptive Query Execution (bom para joins, aggs e skew em geral)\n","spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n","spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n","\n","# Caminho da tabela de destino\n","path_destino = (\n","    \"abfss://ws_departamento_pessoal@onelake.dfs.fabric.microsoft.com/\"\n","    \"lk_departamento_pessoal.Lakehouse/Tables/tab_gold_dim_funcionario_ultima_posicao_mes\"\n",")\n","\n","# Quantidade alvo de linhas por arquivo (ajuste conforme sua realidade)\n","LINHAS_POR_ARQUIVO = 200_000\n","\n","# Horas de retenção para o VACUUM (time travel mínimo preservado)\n","RETENCAO_VACUUM_HORAS = 1\n","\n","# ============================================================\n","# 1. Leitura e Materialização (PERSIST)\n","#    Isola problemas de leitura e evita reprocessar tudo na escrita.\n","# ============================================================\n","\n","print(\"1. Iniciando leitura e carregamento para memória (Persist)...\")\n","start_read = time.time()\n","\n","# Leitura da tabela histórica\n","df_historico = spark.table(\"tab_gold_dim_funcionario_historico\")\n","\n","# Filtro para pegar apenas a última posição do mês (RK_MES = 1)\n","df_filtrado = df_historico.filter(col(\"RK_MES\") == 1)\n","\n","# Persist em memória + disco (mais seguro que cache() se o dataset for grande)\n","df_cached = df_filtrado.persist(StorageLevel.MEMORY_AND_DISK)\n","\n","# Força a execução da leitura e materialização\n","total_linhas = df_cached.count()\n","\n","end_read = time.time()\n","print(f\"   Leitura concluída em {end_read - start_read:.2f} segundos.\")\n","print(f\"   Total de linhas para gravar: {total_linhas}\")\n","\n","# ============================================================\n","# 2. Escrita com Round Robin (A Solução do Skew na saída)\n","# ============================================================\n","\n","if total_linhas > 0:\n","    # Cálculo dinâmico da quantidade de arquivos finais\n","    num_arquivos_finais = builtins.max(1, math.ceil(total_linhas / LINHAS_POR_ARQUIVO))\n","\n","    print(\n","        f\"2. Iniciando gravação Round Robin em {num_arquivos_finais} arquivos \"\n","        f\"(~{LINHAS_POR_ARQUIVO} linhas por arquivo, se possível)...\"\n","    )\n","\n","    (\n","        df_cached\n","            # REPARTITION SEM COLUNAS = ROUND ROBIN\n","            # O Spark distribui as linhas aleatoriamente,\n","            # deixando os arquivos finais bem equilibrados.\n","            .repartition(num_arquivos_finais)\n","            .write\n","            .format(\"delta\")\n","            .mode(\"overwrite\")\n","            .option(\"overwriteSchema\", \"true\")\n","            .save(path_destino)\n","    )\n","\n","    print(\"   Gravação concluída com sucesso.\")\n","else:\n","    print(\"   Nenhum dado encontrado para gravar. Escrita não executada.\")\n","\n","# Libera a memória/disk do cluster\n","df_cached.unpersist()\n","\n","# ============================================================\n","# 3. Manutenção Pós-Gravação (Limpeza com VACUUM)\n","#    Remove arquivos antigos não referenciados, evitando lixo físico.\n","# ============================================================\n","\n","print(\"3. Executando limpeza física (VACUUM)...\")\n","\n","# Desabilita temporariamente a trava de segurança de 7 dias\n","# (use com cuidado; aqui estamos confiando que não há necessidade\n","# de manter versões muito antigas dessa tabela).\n","spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n","\n","delta_table = DeltaTable.forPath(spark, path_destino)\n","\n","# Apaga fisicamente qualquer arquivo que não faça parte das versões\n","# dentro da janela de retenção (ex.: 1 hora).\n","delta_table.vacuum(RETENCAO_VACUUM_HORAS)\n","\n","# Reabilita a trava de segurança\n","spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n","\n","print(\n","    f\"Processo finalizado. Tabela otimizada, com arquivos equilibrados e \"\n","    f\"lixo físico limpo (retenção = {RETENCAO_VACUUM_HORAS} hora(s)).\"\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"500887ec-2bd9-45ad-b4b3-7218fd3fb173","normalized_state":"finished","queued_time":"2025-12-03T12:45:27.2464199Z","session_start_time":null,"execution_start_time":"2025-12-03T12:45:27.2475502Z","execution_finish_time":"2025-12-03T12:45:46.2453067Z","parent_msg_id":"c09c5600-34ee-48cb-a1ec-ce30b8907e97"},"text/plain":"StatementMeta(, 500887ec-2bd9-45ad-b4b3-7218fd3fb173, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1. Iniciando leitura e carregamento para memória (Persist)...\n   Leitura concluída em 0.71 segundos.\n   Total de linhas para gravar: 827268\n2. Iniciando gravação Round Robin em 5 arquivos (~200000 linhas por arquivo, se possível)...\n   Gravação concluída com sucesso.\n3. Executando limpeza física (VACUUM)...\nProcesso finalizado. Tabela otimizada, com arquivos equilibrados e lixo físico limpo (retenção = 1 hora(s)).\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"cellStatus":"{\"Suporte BI\":{\"session_start_time\":null,\"execution_start_time\":\"2025-12-03T12:45:27.2475502Z\",\"execution_finish_time\":\"2025-12-03T12:45:46.2453067Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}","advisor":{"adviceMetadata":"{\"artifactId\":\"eadf2106-caa2-4964-9368-cde1f5fc875f\",\"activityId\":\"500887ec-2bd9-45ad-b4b3-7218fd3fb173\",\"applicationId\":\"application_1764763965919_0001\",\"jobGroupId\":\"13\",\"advices\":{\"warn\":2}}"}},"id":"f625b998-4365-4ea7-89b6-2927afc92ca4"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"2df79f82-2938-4b29-9d25-58c6ffa4aba7"}],"default_lakehouse":"2df79f82-2938-4b29-9d25-58c6ffa4aba7","default_lakehouse_name":"lk_departamento_pessoal","default_lakehouse_workspace_id":"4665fc94-ec39-4b0d-a3cf-570d42f500cc"}}},"nbformat":4,"nbformat_minor":5}