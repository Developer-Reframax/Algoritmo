{"cells":[{"cell_type":"code","source":["import time\n","import math\n","import builtins  # para garantir o uso do max() nativo do Python\n","from pyspark.sql.functions import col\n","from pyspark import StorageLevel\n","from delta.tables import DeltaTable\n","\n","# ============================================================\n","# 0. Configurações de Segurança e Performance\n","# ============================================================\n","\n","# Desliga o optimizeWrite para termos controle total sobre a quantidade de arquivos.\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"false\")\n","\n","# Mantém o AQE ligado (sempre bom).\n","spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n","\n","# Caminho da tabela de destino\n","path_destino = (\n","    \"abfss://ws_departamento_pessoal@onelake.dfs.fabric.microsoft.com/\"\n","    \"lk_departamento_pessoal.Lakehouse/Tables/tab_gold_dim_funcionario_ultima_posicao_mes\"\n",")\n","\n","# Aumentamos o alvo. Se vamos salvar \"milhares\", geralmente queremos \n","# agrupar tudo em 1 ou poucos arquivos. 500k é um bom número para Delta.\n","LINHAS_POR_ARQUIVO = 500_000 \n","\n","RETENCAO_VACUUM_HORAS = 1\n","\n","# ============================================================\n","# 1. Leitura e Materialização\n","# ============================================================\n","\n","print(\"1. Iniciando leitura e filtro...\")\n","start_read = time.time()\n","\n","# Leitura da Tabela Gigante (Milhões de linhas)\n","df_historico = spark.table(\"tab_gold_dim_funcionario_historico\")\n","\n","# O Filtro que reduz drasticamente o volume (O \"Funil\")\n","df_filtrado = df_historico.filter(col(\"RK_MES\") == 1)\n","\n","# Persistimos para garantir que a contagem e a escrita usem os mesmos dados\n","# sem reler a tabela gigante duas vezes.\n","df_cached = df_filtrado.persist(StorageLevel.MEMORY_AND_DISK)\n","\n","# Força o processamento para sabermos o tamanho do resultado\n","total_linhas = df_cached.count()\n","\n","end_read = time.time()\n","print(f\"   Leitura concluída em {end_read - start_read:.2f} segundos.\")\n","print(f\"   Total de linhas resultantes (saída): {total_linhas}\")\n","\n","# ============================================================\n","# 2. Escrita com COALESCE (A Solução do Efeito Funil)\n","# ============================================================\n","\n","if total_linhas > 0:\n","    # Calcula quantos arquivos precisamos (provavelmente será 1)\n","    num_arquivos_finais = builtins.max(1, math.ceil(total_linhas / LINHAS_POR_ARQUIVO))\n","\n","    print(\n","        f\"2. Iniciando gravação Otimizada (Coalesce) em {num_arquivos_finais} arquivo(s)...\"\n","    )\n","\n","    (\n","        df_cached\n","            # ============================================================\n","            # A MUDANÇA CRÍTICA: COALESCE\n","            # ============================================================\n","            # Diferente do repartition(), o coalesce NÃO embaralha os dados\n","            # pela rede. Ele apenas diz às partições: \"Juntem-se!\".\n","            # É perfeito para quando você tem muitas partições de entrada\n","            # e quer poucas de saída.\n","            .coalesce(num_arquivos_finais)\n","            .write\n","            .format(\"delta\")\n","            .mode(\"overwrite\")\n","            .option(\"overwriteSchema\", \"true\")\n","            .save(path_destino)\n","    )\n","\n","    print(\"   Gravação concluída com sucesso.\")\n","else:\n","    print(\"   Nenhum dado encontrado para gravar.\")\n","\n","# Libera recursos\n","df_cached.unpersist()\n","\n","# ============================================================\n","# 3. Limpeza (VACUUM)\n","# ============================================================\n","\n","print(\"3. Executando limpeza física (VACUUM)...\")\n","\n","spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n","delta_table = DeltaTable.forPath(spark, path_destino)\n","delta_table.vacuum(RETENCAO_VACUUM_HORAS)\n","spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n","\n","print(\"Processo finalizado.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"90a291d7-6f36-41d4-ad68-272fe51ac41c","normalized_state":"finished","queued_time":"2025-12-04T14:26:47.0538695Z","session_start_time":"2025-12-04T14:26:47.0551352Z","execution_start_time":"2025-12-04T14:27:00.5044662Z","execution_finish_time":"2025-12-04T14:28:02.5311935Z","parent_msg_id":"fc921c66-7009-47c6-bf87-644837b469c1"},"text/plain":"StatementMeta(, 90a291d7-6f36-41d4-ad68-272fe51ac41c, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["1. Iniciando leitura e filtro...\n   Leitura concluída em 34.41 segundos.\n   Total de linhas resultantes (saída): 827277\n2. Iniciando gravação Otimizada (Coalesce) em 2 arquivo(s)...\n   Gravação concluída com sucesso.\n3. Executando limpeza física (VACUUM)...\nProcesso finalizado.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"cellStatus":"{\"Suporte BI\":{\"session_start_time\":\"2025-12-04T14:26:47.0551352Z\",\"execution_start_time\":\"2025-12-04T14:27:00.5044662Z\",\"execution_finish_time\":\"2025-12-04T14:28:02.5311935Z\",\"state\":\"finished\",\"livy_statement_state\":\"available\",\"normalized_state\":\"finished\"}}","advisor":{"adviceMetadata":"{\"artifactId\":\"eadf2106-caa2-4964-9368-cde1f5fc875f\",\"activityId\":\"90a291d7-6f36-41d4-ad68-272fe51ac41c\",\"applicationId\":\"application_1764857893820_0001\",\"jobGroupId\":\"3\",\"advices\":{\"warn\":1}}"}},"id":"f625b998-4365-4ea7-89b6-2927afc92ca4"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"2df79f82-2938-4b29-9d25-58c6ffa4aba7"}],"default_lakehouse":"2df79f82-2938-4b29-9d25-58c6ffa4aba7","default_lakehouse_name":"lk_departamento_pessoal","default_lakehouse_workspace_id":"4665fc94-ec39-4b0d-a3cf-570d42f500cc"}}},"nbformat":4,"nbformat_minor":5}